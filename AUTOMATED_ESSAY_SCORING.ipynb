{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Automated Essay Scoring\n",
                "\n",
                "This notebook implements a machine learning pipeline to score essays automatically. It uses TF-IDF for feature extraction and a Random Forest Regressor for prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import nltk\n",
                "import re\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
                "import pickle\n",
                "\n",
                "# Download NLTK data\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('punkt_tab')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "data_path = 'archive/training_set_rel3.tsv'\n",
                "df = pd.read_csv(data_path, sep='\\t', encoding='ISO-8859-1')\n",
                "\n",
                "# Display first few rows\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select relevant columns\n",
                "df = df[['essay_id', 'essay_set', 'essay', 'domain1_score']]\n",
                "\n",
                "# Clean text function\n",
                "def clean_text(text):\n",
                "    # Remove non-alphabetic characters\n",
                "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
                "    # Convert to lowercase\n",
                "    text = text.lower()\n",
                "    # Tokenize\n",
                "    tokens = word_tokenize(text)\n",
                "    # Remove stopwords\n",
                "    stop_words = set(stopwords.words('english'))\n",
                "    tokens = [word for word in tokens if word not in stop_words]\n",
                "    return ' '.join(tokens)\n",
                "\n",
                "# Apply cleaning\n",
                "df['cleaned_essay'] = df['essay'].apply(clean_text)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize TF-IDF Vectorizer\n",
                "vectorizer = TfidfVectorizer(max_features=5000)\n",
                "\n",
                "# Fit and transform the essays\n",
                "X = vectorizer.fit_transform(df['cleaned_essay']).toarray()\n",
                "y = df['domain1_score']\n",
                "\n",
                "print(\"Shape of X:\", X.shape)\n",
                "print(\"Shape of y:\", y.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Initialize RandomForest Regressor\n",
                "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
                "\n",
                "# Train the model\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "print(\"Model trained successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict on test set\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Calculate Mean Squared Error\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "print(f\"Mean Squared Error: {mse}\")\n",
                "\n",
                "# Calculate Cohen's Kappa Score (Quadratic Weighted Kappa is standard for essay scoring)\n",
                "# We round predictions to nearest integer for Kappa calculation\n",
                "y_pred_rounded = np.round(y_pred).astype(int)\n",
                "kappa = cohen_kappa_score(y_test.astype(int), y_pred_rounded, weights='quadratic')\n",
                "print(f\"Quadratic Weighted Kappa: {kappa}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model and vectorizer\n",
                "with open('model.pkl', 'wb') as f:\n",
                "    pickle.dump(model, f)\n",
                "\n",
                "with open('vectorizer.pkl', 'wb') as f:\n",
                "    pickle.dump(vectorizer, f)\n",
                "\n",
                "print(\"Model and vectorizer saved to disk.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}